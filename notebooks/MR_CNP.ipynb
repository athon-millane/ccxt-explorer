{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxVTz4-ObQ8t"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 Google LLC.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability.python.layers import util as tfp_layers_util\n",
    "\n",
    "tf.enable_v2_tensorshape()\n",
    "\n",
    "NeuralProcessParams = collections.namedtuple('NeuralProcessParams',\n",
    "                                             ['dim_r', 'dim_z','dim_w',\n",
    "                                              'n_hidden_units_r',\n",
    "                                              'n_hidden_units_g'])\n",
    "GaussianParams = collections.namedtuple('GaussianParams', ['mu', 'sigma'])\n",
    "cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "595zp5kimJQH"
   },
   "outputs": [],
   "source": [
    "#choose the model here\n",
    "exp_type = 'CNP' # choose from 'CNP', 'MR-CNP-A', 'MR-CNP-W'\n",
    "\n",
    "noise_scale = 0.1 #@param {type:\"number\"}\n",
    "n_obs = 20 #@param {type:\"number\"}\n",
    "n_context = 10 #@param {type:\"number\"}\n",
    "K_amp = 20 #@param {type:\"number\"}\n",
    "x_width = 5 #@param {type:\"number\"}\n",
    "learning_rate= 0.001 #@param {type:\"number\"}\n",
    "n_iter = 30000 #@param {type:\"number\"}\n",
    "amps = np.linspace(0.1,4,K_amp)\n",
    "params = \\\n",
    "NeuralProcessParams(dim_r=10, dim_z=5, dim_w = 5,\n",
    "                    n_hidden_units_r=[100,100],\n",
    "                    n_hidden_units_g=[100,100]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AL-Qce4wlNE6"
   },
   "outputs": [],
   "source": [
    "# auxilliary functions\n",
    "\n",
    "def split_context_target(xs, ys, n_context,\n",
    "                         context_xs, context_ys, target_xs, target_ys):\n",
    "    \"\"\"split samples randomly into task training and task validation sets.\n",
    "    \"\"\"\n",
    "    indices = set(range(ys.shape[0]))\n",
    "    context_set_indices = set(random.sample(indices, n_context))\n",
    "    target_set_indices = indices - context_set_indices\n",
    "\n",
    "    return {\n",
    "        context_xs: xs[list(context_set_indices), :],\n",
    "        context_ys: ys[list(context_set_indices), :],\n",
    "        target_xs: xs[list(target_set_indices), :],\n",
    "        target_ys: ys[list(target_set_indices), :]\n",
    "    }\n",
    "\n",
    "def sampling(output):\n",
    "    \"\"\"sample from Gaussian Distribution\n",
    "    \"\"\"\n",
    "    mu, logstd = tf.split(output, num_or_size_splits=2, axis=-1)\n",
    "    sigma = tf.nn.softplus(logstd)\n",
    "    ws = mu + tf.random_normal(tf.shape(mu)) * sigma\n",
    "    return ws, mu, sigma\n",
    "\n",
    "def merge(A, B):\n",
    "    \"\"\"A is [n, k1], B is [m, k2], return [n, m, (k1+k2)]\n",
    "    \"\"\"\n",
    "    A_repeat = tf.expand_dims(A, axis=1)\n",
    "    A_repeat = tf.tile(A_repeat, [1, tf.shape(B)[0], 1])\n",
    "\n",
    "    B_repeat = tf.expand_dims(B, axis=0)\n",
    "    B_repeat = tf.tile(B_repeat, [tf.shape(A)[0], 1, 1])\n",
    "\n",
    "    return tf.concat([A_repeat, B_repeat], axis=2)\n",
    "\n",
    "def kl_qp_gaussian(mu_q, sigma_q, mu_p, sigma_p):\n",
    "    \"\"\"KL(N(mu_q), Diag(sigma_q^2) || N(mu_p), Diag(sigma_p^2))\n",
    "    \"\"\"\n",
    "    sigma2_q = tf.square(sigma_q) + 1e-16\n",
    "    sigma2_p = tf.square(sigma_p) + 1e-16\n",
    "    temp = tf.log(sigma2_p) - tf.log(sigma2_q) - 1.0 + \\\n",
    "            sigma2_q / sigma2_p + tf.square(mu_q - mu_p) / sigma2_p  #N*D\n",
    "    kl = 0.5 * tf.reduce_sum(temp, axis = 1)\n",
    "    return tf.reduce_mean(kl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iuUx4BmvgSil"
   },
   "outputs": [],
   "source": [
    "# (MR-)CNP model\n",
    "\n",
    "if exp_type == 'MR-CNP-W':\n",
    "    kernel_posterior_fn=tfp_layers_util.default_mean_field_normal_fn(\n",
    "        untransformed_scale_initializer=tf.compat.v1.initializers.random_normal(mean=-3., stddev=0.1))\n",
    "    encoder_w = tf.keras.Sequential([\n",
    "          tfp.layers.DenseReparameterization(100, activation=tf.nn.relu, kernel_posterior_fn = kernel_posterior_fn),\n",
    "          tfp.layers.DenseReparameterization(params.dim_w),\n",
    "        ])\n",
    "\n",
    "if exp_type == 'MR-CNP-A':\n",
    "    def encoder_w(xas, params):\n",
    "        hidden_layer = xas\n",
    "        hidden_layer = tf.layers.dense(hidden_layer, 100,\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      name='encoder_w_{}'.format(0),\n",
    "                                      reuse=tf.AUTO_REUSE,\n",
    "                                      kernel_initializer='normal')\n",
    "        # Last layer is simple linear\n",
    "        mu_w = tf.layers.dense(hidden_layer, params.dim_w, name='w_params_mu',\n",
    "                            reuse=tf.AUTO_REUSE, kernel_initializer='normal')\n",
    "\n",
    "        sigma_w = tf.layers.dense(hidden_layer, params.dim_w, name='w_params_sigma',\n",
    "                                reuse=tf.AUTO_REUSE,\n",
    "                                kernel_initializer='normal')\n",
    "        sigma_w = tf.nn.softplus(sigma_w)\n",
    "        return mu_w, sigma_w\n",
    "\n",
    "def encoder_r(xyas, params):\n",
    "    \"\"\"encode task training data.\n",
    "    \"\"\"\n",
    "    hidden_layer = xyas\n",
    "    for i, n_hidden_units in enumerate(params.n_hidden_units_r):\n",
    "        hidden_layer = tf.layers.dense(hidden_layer, n_hidden_units,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       name='encoder_layer_{}'.format(i),\n",
    "                                       reuse=tf.AUTO_REUSE,\n",
    "                                       kernel_initializer='normal')\n",
    "\n",
    "    # Last layer is linear\n",
    "    i = len(params.n_hidden_units_r)\n",
    "    r = tf.layers.dense(hidden_layer, params.dim_r,\n",
    "                        name='encoder_layer_{}'.format(i),\n",
    "                        reuse=tf.AUTO_REUSE,\n",
    "                        kernel_initializer='normal')\n",
    "    return r\n",
    "\n",
    "\n",
    "def xy_to_z_params(xs, ys, amplitude, params, encoder_w=None):\n",
    "    \"\"\"Aggregator of task training data.\n",
    "    i) rs = T1(xas, ys); ii) r = mean(rs) iii)z = T2(r)\n",
    "    \"\"\"\n",
    "  # i)\n",
    "    amplitude = tf.tile(amplitude, [tf.shape(xs)[0], 1])\n",
    "    if exp_type == 'CNP':\n",
    "        xyas = tf.concat([xs, ys, amplitude], axis=1)\n",
    "        rs = encoder_r(xyas, params)\n",
    "\n",
    "    if exp_type == 'MR-CNP-W':\n",
    "        xas = tf.concat([xs, amplitude], axis=1)\n",
    "        ws = encoder_w(xas)\n",
    "        wys = tf.concat([ws, ys], axis=1)\n",
    "        rs = encoder_r(wys, params)\n",
    "\n",
    "    if exp_type == 'MR-CNP-A':\n",
    "        xas = tf.concat([xs, amplitude], axis=1)\n",
    "        mu_w,  sigma_w= encoder_w(xas, params)\n",
    "        ws = mu_w + tf.random_normal(tf.shape(mu_w)) * sigma_w\n",
    "        wys = tf.concat([ws, ys], axis=1)\n",
    "        rs = encoder_r(wys, params)\n",
    "    # ii)\n",
    "    r = tf.reshape(tf.reduce_mean(rs, axis=0), [1, -1])\n",
    "    # iii)\n",
    "    z_sample = tf.layers.dense(r, params.dim_z, name='z_params_mu',\n",
    "                      reuse=tf.AUTO_REUSE, kernel_initializer='normal')\n",
    "    if exp_type == 'MR-CNP-A':\n",
    "        return z_sample, mu_w, sigma_w\n",
    "    else:\n",
    "        return z_sample\n",
    "\n",
    "\n",
    "def decoder_g(zws, params, activation = None):\n",
    "    \"\"\"y_hat =  G(context, task testing input)\n",
    "    \"\"\"\n",
    "    hidden_layer = zws\n",
    "    for i, n_hidden_units in enumerate(params.n_hidden_units_g):\n",
    "        hidden_layer = tf.layers.dense(hidden_layer, n_hidden_units,\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       name='decoder_layer_{}'.format(i),\n",
    "                                       reuse=tf.AUTO_REUSE,\n",
    "                                       kernel_initializer='normal')\n",
    "  \n",
    "    # Last layer is linear\n",
    "    i = len(params.n_hidden_units_g)\n",
    "    y_hat = tf.layers.dense(hidden_layer, 1,\n",
    "                            name='decoder_layer_{}'.format(i),\n",
    "                            reuse=tf.AUTO_REUSE,\n",
    "                            kernel_initializer='normal')\n",
    "    return y_hat\n",
    "  \n",
    "\n",
    "def neg_loglikelihood(y_star, mu_star, noise_scale=noise_scale):\n",
    "    p_normal = tf.distributions.Normal(loc=mu_star, scale=noise_scale)\n",
    "    loglike = p_normal.log_prob(y_star) #n_target * n_z\n",
    "    loglike = tf.reduce_sum(loglike, axis=0)\n",
    "    loglike = tf.reduce_mean(loglike)\n",
    "    return -loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cG4tFtXZmMaT"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5116707d590b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Placeholders for training inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcontext_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcontext_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtarget_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders for training inputs\n",
    "context_xs = tf.placeholder(tf.float32, (None, 1))\n",
    "context_ys = tf.placeholder(tf.float32, (None, 1))\n",
    "target_xs = tf.placeholder(tf.float32, (None, 1))\n",
    "target_ys = tf.placeholder(tf.float32, (None, 1))\n",
    "amplitude = tf.placeholder(tf.float32, (1, K_amp))\n",
    "if exp_type == 'MR-CNP-W':\n",
    "    Beta = tf.placeholder_with_default(0.15, ())\n",
    "if exp_type == 'MR-CNP-A':\n",
    "    Beta = tf.placeholder_with_default(5.0, ())\n",
    "\n",
    "if exp_type == 'MR-CNP-A':\n",
    "    z_samples, mu_w, sigma_w =  xy_to_z_params(context_xs, context_ys, amplitude, params, encoder_w)\n",
    "else:\n",
    "    z_samples =  xy_to_z_params(context_xs, context_ys, amplitude, params, encoder_w)\n",
    "\n",
    "target_xas = tf.concat([target_xs, tf.tile(amplitude, [tf.shape(target_xs)[0], 1])], axis=1)\n",
    "\n",
    "if exp_type == 'CNP':\n",
    "    input_target = merge(z_samples, target_xas)\n",
    "\n",
    "if exp_type == 'MR-CNP-W':\n",
    "    with tf.variable_scope('encoder_w'):\n",
    "        target_ws = encoder_w(target_xas)\n",
    "    input_target = merge(z_samples, target_ws) #n_z * n_target * (d_z + d_w)\n",
    "\n",
    "if exp_type == 'MR-CNP-A':\n",
    "    target_mu_w, target_sigma_w = encoder_w(target_xas, params)\n",
    "    target_ws = target_mu_w + tf.random_normal(tf.shape(target_mu_w)) * target_sigma_w\n",
    "    input_target = merge(z_samples, target_ws) #n_z * n_target * (d_z + d_w)\n",
    "\n",
    "# sample y_hat ~  y|(w,z)\n",
    "y_star_mu_test = decoder_g(input_target, params)\n",
    "y_star_mu_test = tf.squeeze(y_star_mu_test, axis=2)\n",
    "y_star_mu_test = tf.transpose(y_star_mu_test)  #n_target * n_z\n",
    "\n",
    "\n",
    "# loss & optimizer\n",
    "neg_loglike = neg_loglikelihood(target_ys, y_star_mu_test)\n",
    "if exp_type == 'CNP':\n",
    "    loss = neg_loglike\n",
    "if exp_type == 'MR-CNP-W':\n",
    "    kl_loss = Beta * sum(encoder_w.losses)\n",
    "    loss = neg_loglike + kl_loss\n",
    "\n",
    "if exp_type == 'MR-CNP-A':\n",
    "    kl_ib = kl_qp_gaussian(mu_w, sigma_w,\n",
    "                           tf.zeros(tf.shape(mu_w)), tf.ones(tf.shape(mu_w)))\n",
    "    loss = neg_loglike + Beta * kl_ib\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "def train_model(sess, n_iter = n_iter, verbose = False):\n",
    "    cost = []; cost_r = []\n",
    "    for i in range(n_iter):\n",
    "        #generate data\n",
    "        xs = np.random.uniform(-x_width, x_width, n_obs)\n",
    "        amp_ind = random.randint(0,K_amp-5)\n",
    "        amp = np.zeros([1, K_amp]); amp[0,amp_ind] = 1\n",
    "        ys = amps[amp_ind] * np.sin(xs) + np.random.normal(scale = noise_scale, size = xs.shape)\n",
    "        feed_dict = split_context_target(xs.reshape(-1, 1), ys.reshape(-1, 1),\n",
    "                                        n_context, context_xs, context_ys, target_xs, target_ys)\n",
    "        feed_dict.update({amplitude:amp})\n",
    "  \n",
    "        #training\n",
    "        A = sess.run((train_op, loss), feed_dict=feed_dict)\n",
    "        cost.append(A[1])\n",
    "  \n",
    "        if verbose and i%5000 == 0:\n",
    "            cost_r.append(np.mean(cost))\n",
    "            print('iter=', i, \"Loss: {:.3f}\".format(np.mean(cost)))\n",
    "            cost = []\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_cfN_s4mMKB"
   },
   "outputs": [],
   "source": [
    "def plot_out(sess, amp_ind, n_context=n_context, amps = amps, width = x_width, ax = None, seed=None, K_test_sample=100, legend = False):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed**2)\n",
    "    A = np.random.uniform(low = amps[0], high = amps[-1])\n",
    "    a_onehot = np.zeros([1, K_amp]); a_onehot[0,amp_ind] = 1\n",
    "    #task training\n",
    "    xc_test = np.reshape(np.random.uniform(-width, width, n_context),[-1, 1])\n",
    "    yc_test = A*np.sin(xc_test) + np.random.normal(scale = noise_scale, size = xc_test.shape)\n",
    "    #task validation\n",
    "    xs_test = np.reshape(np.linspace(-5, 5, 100),[-1, 1])\n",
    "    ys_test = A*np.sin(xs_test)\n",
    "\n",
    "    mean_curve = 0\n",
    "    for i in range(K_test_sample):\n",
    "        sample_curves = sess.run(y_star_mu_test, feed_dict = {context_xs:xc_test , context_ys:yc_test ,\n",
    "                              target_xs:xs_test, target_ys:ys_test, amplitude:a_onehot})\n",
    "        mean_curve += sample_curves[:,0] / (K_test_sample*1.0)\n",
    "    error = np.mean(np.square(mean_curve - ys_test[:,0]))\n",
    "\n",
    "    if ax is not None:\n",
    "        plt.sca(ax)\n",
    "        plt.plot(xs_test,ys_test,'-', color='darkorange', linewidth=2.0, label = 'True Function', zorder=2)\n",
    "        plt.plot(xs_test,mean_curve,'b-', linewidth=2.0, label = 'Prediction', zorder=2)\n",
    "        plt.ylim([-6,6])\n",
    "        plt.axvline(x=-width, linestyle='--')\n",
    "        plt.axvline(x=width,linestyle='--')\n",
    "        plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "        plt.plot(xc_test,yc_test, '^',  label = 'Context Points', zorder=3)\n",
    "        if legend:\n",
    "          plt.legend( loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    return error, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLZgC-cCr7Ge"
   },
   "source": [
    "## Training & Testing for CNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "77kSeqXgr6fE",
    "outputId": "a0fe9fa6-0764-49df-90a0-9cb724727783"
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess = train_model(sess, n_iter=n_iter, verbose=False)\n",
    "\n",
    "print(\"Model is\", exp_type)\n",
    "n_test_task = 100\n",
    "n_context_test = 5\n",
    "errors = []\n",
    "for i in range(n_test_task):\n",
    "    error, A = plot_out(sess, amp_ind = random.randint(5,K_amp-5), n_context=n_context_test, seed = i)\n",
    "    errors.append(error)\n",
    "print(\"n_context = \", n_context_test, \"error =\", np.mean(errors))\n",
    "\n",
    "n_context_test = 10\n",
    "errors = []\n",
    "for i in range(n_test_task):\n",
    "    error, A = plot_out(sess, amp_ind = random.randint(5,K_amp-5), n_context=n_context_test, seed = i)\n",
    "    errors.append(error)\n",
    "print(\"n_context = \", n_context_test, \"error =\", np.mean(errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "AEq5_IIDt6D-",
    "outputId": "3b522a70-6624-4f56-d10c-af1ab10c8543"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(16,4))\n",
    "a_onehot = 15\n",
    "error, A = plot_out(sess, a_onehot, seed = 1,  ax = ax[0]);\n",
    "error, A  = plot_out(sess, a_onehot, seed = 3,  ax = ax[1]);\n",
    "error, A = plot_out(sess, a_onehot, seed = 5,  ax = ax[2]);\n",
    "error, A = plot_out(sess, a_onehot,  seed = 15,  ax = ax[3], legend='True');\n",
    "for ax_i in ax:\n",
    "    ax_i.tick_params(axis='both', which='major',length=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Prpp-yz_csuz"
   },
   "source": [
    "## Training & Testing for MR-CNP(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ufOYpg9Pvddd",
    "outputId": "f52b4291-f472-45c1-cbbd-5847152da92e"
   },
   "outputs": [],
   "source": [
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess = train_model(sess, n_iter=n_iter, verbose=False)\n",
    "\n",
    "\n",
    "print(\"Model is\", exp_type)\n",
    "n_test_task = 100\n",
    "n_context_test = 5\n",
    "errors = []\n",
    "for i in range(n_test_task):\n",
    "    error, A = plot_out(sess, amp_ind = random.randint(5,K_amp-5), n_context=n_context_test, seed = i)\n",
    "    errors.append(error)\n",
    "print(\"n_context = \", n_context_test, \"error =\", np.mean(errors))\n",
    "\n",
    "n_context_test = 10\n",
    "errors = []\n",
    "for i in range(n_test_task):\n",
    "    error, A = plot_out(sess, amp_ind = random.randint(5,K_amp-5), n_context=n_context_test, seed = i)\n",
    "    errors.append(error)\n",
    "print(\"n_context = \", n_context_test, \"error =\", np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "tJHasbZic4O_",
    "outputId": "d4df6086-ed2f-4cb9-d030-de9e10af99aa"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(16,4))\n",
    "a_onehot = 15\n",
    "error, A = plot_out(sess, a_onehot, seed = 1,  ax = ax[0]);\n",
    "error, A  = plot_out(sess, a_onehot, seed = 3,  ax = ax[1]);\n",
    "error, A = plot_out(sess, a_onehot, seed = 5,  ax = ax[2]);\n",
    "error, A = plot_out(sess, a_onehot,  seed = 15,  ax = ax[3], legend='True');\n",
    "for ax_i in ax:\n",
    "    ax_i.tick_params(axis='both', which='major',length=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wEhBbmxJc6qK"
   },
   "source": [
    "#Training & Testing for MR-CNP(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "qkZWD0huc94j",
    "outputId": "c3c6715b-f362-4e77-b206-7b55876bfadf"
   },
   "outputs": [],
   "source": [
    "n_iter = 120000\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess = train_model(sess, n_iter=n_iter, verbose=False)\n",
    "\n",
    "\n",
    "print(\"Model is\", exp_type)\n",
    "n_test_task = 100\n",
    "n_context_test = 5\n",
    "errors = []\n",
    "for i in range(n_test_task):\n",
    "    error, A = plot_out(sess, amp_ind = random.randint(5,K_amp-5), n_context=n_context_test, seed = i)\n",
    "    errors.append(error)\n",
    "print(\"n_context = \", n_context_test, \"error =\", np.mean(errors))\n",
    "\n",
    "n_context_test = 10\n",
    "errors = []\n",
    "for i in range(n_test_task):\n",
    "    error, A = plot_out(sess, amp_ind = random.randint(5,K_amp-5), n_context=n_context_test, seed = i)\n",
    "    errors.append(error)\n",
    "print(\"n_context = \", n_context_test, \"error =\", np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "Rm2IK3NEc9rO",
    "outputId": "e9c3357c-3ca5-447d-be0f-c6b51798ae3c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(16,4))\n",
    "a_onehot = 15\n",
    "error, A = plot_out(sess, a_onehot, seed = 1,  ax = ax[0]);\n",
    "error, A  = plot_out(sess, a_onehot, seed = 3,  ax = ax[1]);\n",
    "error, A = plot_out(sess, a_onehot, seed = 5,  ax = ax[2]);\n",
    "error, A = plot_out(sess, a_onehot,  seed = 15,  ax = ax[3], legend='True');\n",
    "for ax_i in ax:\n",
    "    ax_i.tick_params(axis='both', which='major',length=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj_JLympfobO"
   },
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(MR-)CNP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:rltrade]",
   "language": "python",
   "name": "conda-env-rltrade-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
